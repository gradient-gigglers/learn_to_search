{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "75002b63",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import sentencepiece as spm\n",
    "from datasets import load_dataset\n",
    "\n",
    "\n",
    "#dataset = load_dataset(\"ade_corpus_v2\", \"Ade_corpus_v2_classification\")\n",
    "# Extract text from the dataset\n",
    "#text_data = dataset[\"train\"][\"text\"]\n",
    "\n",
    "# Define the parameters for training\n",
    "#spm.SentencePieceTrainer.Train(\n",
    "    #input=text_data,  # Path to your training data\n",
    "    #model_prefix='your_model',  # Prefix for model files\n",
    "    #vocab_size=8000,  # Adjust based on your needs\n",
    "    #character_coverage=0.98,  # Adjust based on your needs\n",
    "    #model_type='unigram',  # Model type (unigram, bpe, char, or word)\n",
    "#)\n",
    "# Define a function to load a SentencePiece model and encode text\n",
    "#def load_sentencepiece_model(model_file):\n",
    "    #sp = spm.SentencePieceProcessor()\n",
    "    #sp.load(model_file)\n",
    "    #return sp\n",
    "\n",
    "\n",
    "# Sample data: Triplets (query, positive_doc, negative_doc)\n",
    "sample_data = [\n",
    "    (\"Tell me about the Eiffel Tower.\", \"The Eiffel Tower is an iconic landmark in Paris.\", \"Apples are a type of fruit.\"),\n",
    "    (\"What is the capital of France?\", \"Paris is the capital of France.\", \"Bananas are a yellow fruit.\"),\n",
    "    (\"Explain the theory of relativity.\", \"The theory of relativity was developed by Albert Einstein.\", \"Dogs are loyal animals.\"),\n",
    "    (\"Tell me about the Mona Lisa.\", \"The Mona Lisa is a famous painting by Leonardo da Vinci.\", \"Cats are independent animals.\"),\n",
    "    (\"What is the largest planet in our solar system?\", \"Jupiter is the largest planet in our solar system.\", \"Elephants are the largest land mammals.\"),\n",
    "    (\"Who wrote the play 'Romeo and Juliet'?\", \"William Shakespeare wrote the play 'Romeo and Juliet'.\", \"Roses are a type of flower.\"),\n",
    "    (\"What is the capital of Japan?\", \"Tokyo is the capital of Japan.\", \"Fish are cold-blooded animals.\"),\n",
    "    (\"Explain the concept of photosynthesis.\", \"Photosynthesis is the process by which plants convert sunlight into energy.\", \"Owls are nocturnal birds.\"),\n",
    "    (\"Tell me about the Great Wall of China.\", \"The Great Wall of China is a historic fortification in China.\", \"Kangaroos are marsupials.\"),\n",
    "    (\"Who is the current President of the United States?\", \"Joe Biden is the current President of the United States.\", \"Tigers are large cats.\")\n",
    "]\n",
    "\n",
    "# Define the queries, positive_docs, and negative_docs lists\n",
    "queries = [item[0] for item in sample_data]\n",
    "positive_docs = [item[1] for item in sample_data]\n",
    "negative_docs = [item[2] for item in sample_data]\n",
    "\n",
    "# Load a pre-trained SentencePiece model and tokenize the data\n",
    "#sp_model = load_sentencepiece_model(\"your_model.model\")\n",
    "#queries = [sp_model.encode_as_ids(query) for query in queries]\n",
    "#positive_docs = [sp_model.encode_as_ids(doc) for doc in positive_docs]\n",
    "#negative_docs = [sp_model.encode_as_ids(doc) for doc in negative_docs]\n",
    "\n",
    "vocab = {\"<UNK>\", \"<PAD>\"}\n",
    "\n",
    "# Populate the vocab from the data\n",
    "for text in positive_docs + queries + negative_docs:\n",
    "    words = text.split()\n",
    "    vocab.update(words)\n",
    "\n",
    "# Create a word-to-index mapping\n",
    "word_to_idx = {word: idx for idx, word in enumerate(vocab)}\n",
    "\n",
    "# Helper function to convert text to sequence of word indices\n",
    "def text_to_sequence(text):\n",
    "    return [word_to_idx.get(word, word_to_idx[\"<UNK>\"]) for word in text.split()]\n",
    "\n",
    "# Convert paragraphs and queries into sequences of word indices\n",
    "p_paragraph_sequences = [text_to_sequence(paragraph) for paragraph in positive_docs]\n",
    "n_paragraph_sequences = [text_to_sequence(paragraph) for paragraph in negative_docs]\n",
    "query_sequences = [text_to_sequence(query) for query in queries]\n",
    "\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Define the Three-Tower LSTM model\n",
    "class SiameseLSTM(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_layers):\n",
    "        super(SiameseLSTM, self).__init__() \n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.query_lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers, batch_first=True, bidirectional=True)\n",
    "        self.positive_lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers, batch_first=True, bidirectional=True)\n",
    "        self.negative_lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers, batch_first=True, bidirectional=True)\n",
    "        self.cosine_similarity = nn.CosineSimilarity(dim=1, eps=1e-6)\n",
    "\n",
    "    def forward(self, query, positive_doc, negative_doc):\n",
    "        query_embedding = self.embedding(query)\n",
    "        positive_embedding = self.embedding(positive_doc)\n",
    "        negative_embedding = self.embedding(negative_doc)\n",
    "\n",
    "        query_embedding, _= self.query_lstm(query_embedding)\n",
    "        positive_embedding, _ = self.positive_lstm(positive_embedding)\n",
    "        negative_embedding, _ = self.negative_lstm(negative_embedding)\n",
    "        \n",
    "        positive_similarity = self.cosine_similarity(query_embedding, positive_embedding)\n",
    "        negative_similarity = self.cosine_similarity(query_embedding, negative_embedding)\n",
    "\n",
    "\n",
    "        return positive_similarity, negative_similarity\n",
    "    \n",
    "            \n",
    "    def contrastive_loss(self, positive_similarity, negative_similarity, margin=0.5):\n",
    "        # The goal is to make positive similarity larger and negative similarity smaller\n",
    "        loss = F.relu(margin - positive_similarity + negative_similarity).mean()\n",
    "        return loss\n",
    "\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f5cd10c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the maximum sequence length across all documents and queries\n",
    "max_seq_length = max(max(len(seq) for seq in p_paragraph_sequences + n_paragraph_sequences), max(len(seq) for seq in query_sequences))\n",
    "\n",
    "# Pad all sequences to this maximum length\n",
    "def pad_sequence(seq, max_length):\n",
    "    return seq + [0] * (max_length - len(seq))\n",
    "\n",
    "padded_query_sequences = [pad_sequence(seq, max_seq_length) for seq in query_sequences]\n",
    "padded_p_paragraph_sequences = [pad_sequence(seq, max_seq_length) for seq in p_paragraph_sequences]\n",
    "padded_n_paragraph_sequences = [pad_sequence(seq, max_seq_length) for seq in n_paragraph_sequences]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "79f76ac5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_seq_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fb639665",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5, Mean Loss: 0.5771744608879089\n",
      "Epoch 2/5, Mean Loss: 0.4293012320995331\n",
      "Epoch 3/5, Mean Loss: 0.3462718665599823\n",
      "Epoch 4/5, Mean Loss: 0.2857722580432892\n",
      "Epoch 5/5, Mean Loss: 0.2384976327419281\n"
     ]
    }
   ],
   "source": [
    "vocab_size = len(vocab) \n",
    "embedding_dim = 50\n",
    "hidden_dim = 64\n",
    "num_layers = 2\n",
    "three_tower_model = SiameseLSTM(vocab_size, embedding_dim, hidden_dim, num_layers)\n",
    "\n",
    "# Define an optimizer\n",
    "optimizer = optim.SGD(three_tower_model.parameters(), lr=0.01)\n",
    "\n",
    "# Training loop (you'll need to define your training data and loops)\n",
    "batch_size = 3\n",
    "total_loss = 0.0\n",
    "total_iterations = len(query_sequences)\n",
    "num_epochs = 5\n",
    "for epoch in range(num_epochs):\n",
    "    for i in range(len(query_sequences)):\n",
    "        query = torch.tensor(padded_query_sequences[i]).unsqueeze(0)\n",
    "        positive_doc = torch.tensor(padded_p_paragraph_sequences[i]).unsqueeze(0)\n",
    "        negative_doc = torch.tensor(padded_n_paragraph_sequences[i]).unsqueeze(0)\n",
    "\n",
    "        positive_similarity, negative_similarity = three_tower_model(query, positive_doc, negative_doc)\n",
    "        loss = three_tower_model.contrastive_loss(positive_similarity, negative_similarity)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    # Optionally, you can print the mean loss for the current epoch\n",
    "    mean_loss = total_loss / total_iterations\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}, Mean Loss: {mean_loss}\")\n",
    "    total_loss = 0.0\n",
    "# Reset total_loss for the next epoch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "af0e0ae4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n"
     ]
    }
   ],
   "source": [
    "# Sample data: Three queries and three sets of relevant documents\n",
    "query = \"What's the theory of relativity?\"\n",
    "\n",
    "relevant_docs = [\n",
    "    \"Relativity was by Einstein.\",\n",
    "    \"Theory explains spacetime.\",\n",
    "    \"Albert discussed relativity.\",\n",
    "    \"Physics changed with relativity.\",\n",
    "    \"Einstein's groundbreaking theory.\"\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "query_sequence = text_to_sequence(query)\n",
    "relevant_doc_sequences = [text_to_sequence(doc) for doc in relevant_docs]\n",
    "\n",
    "\n",
    "\n",
    "# Pad the sequences\n",
    "padded_query_sequence = pad_sequence(query_sequence, max_seq_length)\n",
    "padded_relevant_doc_sequences = [pad_sequence(seq, max_seq_length) for seq in relevant_doc_sequences]\n",
    "\n",
    "# Check lengths of sequences\n",
    "for seq in padded_relevant_doc_sequences:\n",
    "    print(len(seq))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7b680e32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rank 1: Document=Albert discussed relativity. Similarity Score=0.269143670797348\n",
      "Rank 2: Document=Theory explains spacetime. Similarity Score=0.24998407065868378\n",
      "Rank 3: Document=Relativity was by Einstein. Similarity Score=0.24208283424377441\n"
     ]
    }
   ],
   "source": [
    "# Sample data: Three queries and three sets of relevant documents\n",
    "query = \"What's the theory of relativity?\"\n",
    "\n",
    "relevant_docs = [\n",
    "    \"Relativity was by Einstein.\",\n",
    "    \"Theory explains spacetime.\",\n",
    "    \"Albert discussed relativity.\"\n",
    "]\n",
    "\n",
    "\n",
    "query_sequence = text_to_sequence(query)\n",
    "relevant_doc_sequences = [text_to_sequence(doc) for doc in relevant_docs]\n",
    "\n",
    "\n",
    "\n",
    "# Pad the sequences\n",
    "padded_query_sequence = pad_sequence(query_sequence, max_seq_length)\n",
    "padded_relevant_doc_sequences = [pad_sequence(seq, max_seq_length) for seq in relevant_doc_sequences]\n",
    "\n",
    "with torch.no_grad():\n",
    "    similarity_scores = [] \n",
    "    for doc_tensor in padded_relevant_doc_sequences:\n",
    "        negative_doc = torch.tensor(padded_n_paragraph_sequences[1]).unsqueeze(0)\n",
    "        query_tensor = torch.tensor(padded_query_sequence).unsqueeze(0)\n",
    "        positive_tensor = torch.tensor(doc_tensor).unsqueeze(0)\n",
    "        \n",
    "        # Use your model to get similarity scores\n",
    "        positive_similarity,_ = three_tower_model(query_tensor, positive_tensor, negative_doc)\n",
    "        \n",
    "        \n",
    "        \n",
    "        similarity_scores.append(positive_similarity.mean().item())\n",
    "        \n",
    "        \n",
    "ranked_docs = sorted(zip(similarity_scores, relevant_docs), key=lambda x: x[0], reverse=True)\n",
    "\n",
    "# Print the ranked documents\n",
    "for rank, (score,doc) in enumerate(ranked_docs, start=1):\n",
    "    print(f\"Rank {rank}: Document={doc} Similarity Score={score}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23e940d4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
