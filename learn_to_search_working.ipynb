{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7b03d929",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sentencepiece as spm\n",
    "# Replace 'your_file.parquet' with the path to your Parquet file\n",
    "df = pd.read_parquet('0000.parquet')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e24a8b96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>answers</th>\n",
       "      <th>passages</th>\n",
       "      <th>query</th>\n",
       "      <th>query_id</th>\n",
       "      <th>query_type</th>\n",
       "      <th>wellFormedAnswers</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[Yes]</td>\n",
       "      <td>{'is_selected': [0, 0, 1, 0, 0, 0, 0], 'passag...</td>\n",
       "      <td>does human hair stop squirrels</td>\n",
       "      <td>0</td>\n",
       "      <td>description</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[Fossil fuels are basically the remains of ani...</td>\n",
       "      <td>{'is_selected': [0, 1, 0, 0, 0, 0, 0, 0, 0], '...</td>\n",
       "      <td>what are the benefits of fossil fuels</td>\n",
       "      <td>1</td>\n",
       "      <td>description</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[The apothem of a regular polygon is a line se...</td>\n",
       "      <td>{'is_selected': [0, 0, 0, 0, 0, 1, 0, 0, 0], '...</td>\n",
       "      <td>what is a apothem</td>\n",
       "      <td>2</td>\n",
       "      <td>description</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[$45 to $210. 2]</td>\n",
       "      <td>{'is_selected': [0, 0, 0, 0, 0, 1, 0, 0, 0], '...</td>\n",
       "      <td>average cost for custom canopy</td>\n",
       "      <td>3</td>\n",
       "      <td>numeric</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[It is the collection of physical elements tha...</td>\n",
       "      <td>{'is_selected': [0, 1, 0, 0, 0, 0, 0, 0, 0, 0]...</td>\n",
       "      <td>what is a hardware in a computer</td>\n",
       "      <td>4</td>\n",
       "      <td>description</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             answers  \\\n",
       "0                                              [Yes]   \n",
       "1  [Fossil fuels are basically the remains of ani...   \n",
       "2  [The apothem of a regular polygon is a line se...   \n",
       "3                                   [$45 to $210. 2]   \n",
       "4  [It is the collection of physical elements tha...   \n",
       "\n",
       "                                            passages  \\\n",
       "0  {'is_selected': [0, 0, 1, 0, 0, 0, 0], 'passag...   \n",
       "1  {'is_selected': [0, 1, 0, 0, 0, 0, 0, 0, 0], '...   \n",
       "2  {'is_selected': [0, 0, 0, 0, 0, 1, 0, 0, 0], '...   \n",
       "3  {'is_selected': [0, 0, 0, 0, 0, 1, 0, 0, 0], '...   \n",
       "4  {'is_selected': [0, 1, 0, 0, 0, 0, 0, 0, 0, 0]...   \n",
       "\n",
       "                                   query  query_id   query_type  \\\n",
       "0         does human hair stop squirrels         0  description   \n",
       "1  what are the benefits of fossil fuels         1  description   \n",
       "2                      what is a apothem         2  description   \n",
       "3         average cost for custom canopy         3      numeric   \n",
       "4       what is a hardware in a computer         4  description   \n",
       "\n",
       "  wellFormedAnswers  \n",
       "0                []  \n",
       "1                []  \n",
       "2                []  \n",
       "3                []  \n",
       "4                []  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8fb04e13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document 0: Selected 0 | Text: The biggest advantage of using fossil fuels is that they can be easily stored and transported from one place to another. Large reserves of coal are therefore taken from the coal mines to the industries which are acres away from the mines. The petroleum is also taken to too far off power stations to produce energy. Fossil fuels are the highest producers of calorific value in terms of energy. This is also one of the reasons why they are still preferred over the renewable sources of energy or the alternative source\n",
      "Document 1: Selected 1 | Text: Benefits of fossil fuels. Fossil fuels are basically the remains of animals and plants and these are good energy resources. The three main fossil fuels are natural gas, oil, and coal. Fossil fuels are low in cost and are very important resources for our economy. Fossil fuels are used to generate electricity used as fuels for transportation.\n",
      "Document 2: Selected 0 | Text: Fossil fuels are energy resources that come from the remains of plants and animals. These remains are millions of years old. There are three fossil fuels: petroleum oil, natural gas, and coal. Fossil fuels, like coal, oil, and natural gas, provide the energy that powers our lifestyles and our economy. Fossil fuels power everything from the planes in the sky to the cars on the road. They heat our homes and light up the night. Theyre the bedrock we base our energy mix on.\n",
      "Document 3: Selected 0 | Text: Advantages of Fossil Fuels. A major advantage of fossil fuels is their capacity to generate huge amounts of electricity in just a single location. Fossil fuels are very easy to find. When coal is used in power plants, they are very cost effective. Coal is also in abundant supply. Transporting oil and gas to the power stations can be made through the use of pipes making it an easy task. Disadvantages of Fossil Fuels Pollution is a major disadvantage of fossil fuels. This is because they give off carbon dioxide when burned thereby causing a greenhouse effect. This is also the main contributory factor to the global warming experienced by the earth today.\n",
      "Document 4: Selected 0 | Text: Benefits One of the biggest benefits of fossil fuels is their cost. Coal, oil and natural gas are abundant right now and relatively inexpensive to drill or mine for. In fact, coal is the most plentiful fossil fuel and it is found over much of the world. Fossil fuels, like coal, oil, and natural gas, provide the energy that powers our lifestyles and our economy. Fossil fuels power everything from the planes in the sky to the cars on the road. They heat our homes and light up the night. Theyre the bedrock we base our energy mix on.\n",
      "Document 5: Selected 0 | Text: Fossil fuels also generate a lot of money and are a good source of revenue to many countries. They also help related industries provide jobs for people across the globe which is good for the overall economy. Fossil fuels are used for both residential and commercial purposes. The three main fossil fuels are natural gas, oil, and coal. Fossil fuels are low in cost and are very important resources for our economy. Fossil fuels are used to generate electricity used as fuels for transportation.\n",
      "Document 6: Selected 0 | Text: 1. Easily Available: Since these fossil fuels have been of such a great source of energy, more and more extractions are going on every day. The geologists all around the world are trying to find out mines of coals. The pressure is even more as the population is increasing day by day. Fossil fuels are the highest producers of calorific value in terms of energy. This is also one of the reasons why they are still preferred over the renewable sources of energy or the alternative source\n",
      "Document 7: Selected 0 | Text: You have undoubtedly heard of fuels such as coal, oil and natural gas. These are the three main types of fossil fuels. You rely on fossil fuels every day for such tasks as fueling your car and heating your home. And, it's very likely that the electricity in your home comes from a power plant that uses fossil fuels. Let's review. Fossil fuels are sources of energy that formed from the accumulated remains of living organisms that were buried millions of years ago. Pressure, heat and time allow the organic matter to transform into one of the three major types of fossil fuels, which are coal, oil and natural gas.\n",
      "Document 8: Selected 0 | Text: Fossil fuels are sources of energy that have developed within the earth over millions of years. Because fossil fuels-oil, natural gas, and coal-take so long to form, they are considered nonrenewable. Let's review. Fossil fuels are sources of energy that formed from the accumulated remains of living organisms that were buried millions of years ago. Pressure, heat and time allow the organic matter to transform into one of the three major types of fossil fuels, which are coal, oil and natural gas.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "def extract_passages_and_ground_truth(data):\n",
    "    # Check if data is a string and parse it as JSON; otherwise, use it directly if it's a dictionary\n",
    "    if isinstance(data, str):\n",
    "        parsed_json = json.loads(data)\n",
    "    elif isinstance(data, dict):\n",
    "        parsed_json = data\n",
    "    else:\n",
    "        raise ValueError(\"Input must be a JSON string or a dictionary.\")\n",
    "    \n",
    "    # Extract passages and ground truth labels assuming the data is now a dictionary\n",
    "    passages = parsed_json['passage_text']\n",
    "    ground_truth_labels = parsed_json['is_selected']\n",
    "    \n",
    "    # Split each passage into paragraphs, clean them, and pair them with ground truth labels\n",
    "    cleaned_passages_with_labels = []\n",
    "    for passage, label in zip(passages, ground_truth_labels):\n",
    "        paragraphs = passage.split('\", \"')\n",
    "        cleaned_paragraphs = [paragraph.strip() for paragraph in paragraphs]\n",
    "        for paragraph in cleaned_paragraphs:\n",
    "            cleaned_passages_with_labels.append((paragraph, label))\n",
    "    \n",
    "    return cleaned_passages_with_labels\n",
    "\n",
    "# Assuming 'data' is your JSON or dictionary input\n",
    "# This will now create a column with tuples of (cleaned_paragraph, is_selected)\n",
    "df['cleaned_paragraphs_with_labels'] = df['passages'].apply(extract_passages_and_ground_truth)\n",
    "\n",
    "# Now you can iterate over this new column to get both the text and the ground truth\n",
    "for index, (cleaned_paragraph, is_selected) in enumerate(df['cleaned_paragraphs_with_labels'][1]):\n",
    "    print(f\"Document {index}: Selected {is_selected} | Text: {cleaned_paragraph}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2f31584e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_passages_and_labels(data):\n",
    "    if isinstance(data, str):\n",
    "        parsed_json = json.loads(data)\n",
    "    elif isinstance(data, dict):\n",
    "        parsed_json = data\n",
    "    else:\n",
    "        raise ValueError(\"Input must be a JSON string or a dictionary.\")\n",
    "    \n",
    "    # Extract the relevance labels\n",
    "    is_selected = parsed_json.get('is_selected', [])\n",
    "    relevant_indexes = [index for index, value in enumerate(is_selected) if value == 1]\n",
    "    \n",
    "    return relevant_indexes\n",
    "df['cleaned_passages_with_labels'] = df['passages'].apply(extract_passages_and_labels)\n",
    "#df['cleaned_passages_with_labels'][14]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1b6972d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indices with empty or invalid entries: [14, 32, 40, 127, 133, 147, 176, 201, 218, 223, 242, 256, 311, 312, 333, 344, 350, 356, 385, 437, 444, 567, 577, 590, 644, 646, 695, 697, 717, 861, 879, 882, 925, 929, 1014, 1033, 1057, 1204, 1226, 1259, 1283, 1307, 1326, 1333, 1341, 1368, 1381, 1411, 1441, 1449, 1452, 1468, 1487, 1635, 1650, 1699, 1750, 1751, 1753, 1770, 1812, 1905, 1916, 1972, 1981, 2019, 2047, 2056, 2063, 2073, 2083, 2112, 2125, 2127, 2142, 2168, 2243, 2253, 2257, 2314, 2326, 2330, 2433, 2451, 2508, 2547, 2590, 2704, 2723, 2817, 2818, 2891, 2946, 2960, 3063, 3075, 3119, 3123, 3140, 3157, 3190, 3243, 3244, 3264, 3324, 3344, 3345, 3348, 3357, 3392, 3412, 3421, 3462, 3478, 3484, 3494, 3496, 3515, 3558, 3602, 3613, 3637, 3646, 3699, 3716, 3733, 3817, 3826, 3858, 3859, 3882, 3886, 3897, 3952, 4022, 4036, 4061, 4070, 4135, 4143, 4176, 4193, 4315, 4351, 4419, 4420, 4534, 4548, 4571, 4593, 4599, 4652, 4668, 4681, 4706, 4707, 4712, 4759, 4778, 4873, 4897, 4919, 4937, 4951, 4962, 5007, 5052, 5055, 5058, 5061, 5088, 5092, 5156, 5186, 5227, 5233, 5252, 5351, 5404, 5455, 5495, 5519, 5527, 5536, 5552, 5653, 5661, 5666, 5705, 5715, 5743, 5775, 5796, 5810, 5817, 5818, 5862, 5875, 5951, 6108, 6129, 6137, 6146, 6160, 6174, 6183, 6195, 6202, 6258, 6269, 6287, 6294, 6312, 6316, 6339, 6349, 6351, 6390, 6436, 6634, 6637, 6656, 6671, 6728, 6759, 6817, 6882, 6894, 6903, 6965, 7044, 7056, 7092, 7135, 7149, 7218, 7237, 7278, 7283, 7334, 7446, 7451, 7471, 7527, 7639, 7674, 7715, 7793, 7808, 7862, 7887, 8029, 8119, 8179, 8186, 8188, 8193, 8231, 8289, 8301, 8308, 8311, 8326, 8375, 8378, 8468, 8603, 8629, 8636, 8670, 8677, 8698, 8705, 8714, 8760, 8815, 8867, 8894, 8922, 8982, 8988, 8997, 9023, 9047, 9082, 9086, 9095, 9122, 9133, 9183, 9202, 9251, 9252, 9356, 9360, 9368, 9431, 9453, 9479, 9502, 9515, 9538, 9595, 9620, 9643]\n"
     ]
    }
   ],
   "source": [
    "empty_or_invalid_entries = [i for i, docs in enumerate(df['cleaned_passages_with_labels']) if not isinstance(docs, list) or not docs]\n",
    "print(f\"Indices with empty or invalid entries: {empty_or_invalid_entries}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "04f9712c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import json\n",
    "\n",
    "def extract_passages(data):\n",
    "    # Check if data is a string and parse it as JSON; otherwise, use it directly if it's a dictionary\n",
    "    if isinstance(data, str):\n",
    "        parsed_json = json.loads(data)\n",
    "    elif isinstance(data, dict):\n",
    "        parsed_json = data\n",
    "    else:\n",
    "        raise ValueError(\"Input must be a JSON string or a dictionary.\")\n",
    "    \n",
    "    # Extract passages assuming the data is now a dictionary\n",
    "    passages = parsed_json['passage_text']\n",
    "    \n",
    "    # Split each passage into paragraphs, clean them, and return them as a list\n",
    "    cleaned_passages = []\n",
    "    for passage in passages:\n",
    "        paragraphs = passage.split('\", \"')\n",
    "        cleaned_paragraphs = [paragraph.strip() for paragraph in paragraphs]\n",
    "        cleaned_passages.extend(cleaned_paragraphs)\n",
    "    \n",
    "    return cleaned_passages\n",
    "\n",
    "# Assuming 'data' is your JSON or dictionary input\n",
    "df['cleaned_paragraphs'] = df['passages'].apply(extract_passages)\n",
    "\n",
    "# 'cleaned_paragraphs' in the DataFrame now contains lists of paragraphs\n",
    "\n",
    "# To print each paragraph with a pause in between:\n",
    "#for index, paragraphs in enumerate(df['cleaned_paragraphs']):\n",
    "    #for paragraph in paragraphs:\n",
    "        #print(f\"Passage {index+1}, Paragraph:\\n{paragraph}\\n\")\n",
    "        #input(\"Press Enter to continue to the next paragraph...\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73d69ed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, item in enumerate(df['query']):\n",
    "    print(f\"Item {index+1}:\\n{item}\\n\")\n",
    "    # You can use input() to pause after each item, or remove it to print continuously\n",
    "    input(\"Press Enter to continue to the next item...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c4b1b89e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tokenizer as tknz\n",
    "sp = spm.SentencePieceProcessor()\n",
    "sp.load('m.model')\n",
    "tokenizer = tknz.Tokenizer('0000.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "600732d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 0\n",
    "for passage_list in df['cleaned_paragraphs']:\n",
    "    for paragraph in passage_list:\n",
    "        # Assume `tokenizer.encode` returns a list of tokens.\n",
    "        encoded_paragraph = tokenizer.encode(paragraph)\n",
    "        max_length = max(max_length, len(encoded_paragraph))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "db0851c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "368\n"
     ]
    }
   ],
   "source": [
    "print(max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d6e4556f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_and_pad(text, max_length, pad_token_id, sp):\n",
    "    encoded_text = tokenizer.encode(text)\n",
    "    padded_text = encoded_text + [pad_token_id] * (max_length - len(encoded_text)),  # Pad at the end\n",
    "        \n",
    "    return padded_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "65e88b59",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pad_token_id = sp.PieceToId('<pad>')\n",
    "max_length = 368\n",
    "\n",
    "#padded_passages =encode_and_pad(paragraph, max_length, pad_token_id, sp) for paragraph in sublist for sublist in df['tokenized_passages']\n",
    "                      \n",
    "\n",
    "#padded_queries = df['tokenized_queries'].apply(encode_and_pad(query, max_length, pad_token_id, sp))\n",
    "df['tp_queries'] = df['query'].apply(lambda x: encode_and_pad(x, max_length, pad_token_id, tokenizer))   \n",
    "df['tp_passages'] = df['cleaned_paragraphs'].apply(\n",
    "    lambda paragraphs: [encode_and_pad(paragraph, max_length, pad_token_id, tokenizer) for paragraph in paragraphs]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9d2d7bcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "padded_passages = df['tp_passages']\n",
    "#print(padded_passages[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6064f74e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The DataFrame has 9650 rows.\n"
     ]
    }
   ],
   "source": [
    "padded_queries = df['tp_queries'] \n",
    "number_of_rows = len(padded_queries.index)\n",
    "print(f\"The DataFrame has {number_of_rows} rows.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "048a5697",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Flatten the list of lists of tokens\n",
    "flat_list_of_tokens = [token for sublist in df['tp_passages'].explode() for token in sublist]\n",
    "flat_list_of_tokens += [token for token in df['tp_queries']]\n",
    "\n",
    "flat_list_of_tokens = [token for sublist in flat_list_of_tokens for token in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b83a3d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(\"Structure of the first document:\", padded_passages[0][0])\n",
    "\n",
    "# Attempt to flatten the first document if it's a list of lists\n",
    "flat_doc = [item for sublist in padded_passages[0][0] for item in sublist]\n",
    "\n",
    "# Check the flattened document\n",
    "print(\"Flattened document:\", flat_doc)\n",
    "\n",
    "# Try to convert the flattened document to a tensor\n",
    "try:\n",
    "    doc_tensor = torch.tensor([flat_doc], dtype=torch.long)\n",
    "    print(\"Tensor created successfully.\")\n",
    "except Exception as e:\n",
    "    print(\"An error occurred while creating the tensor:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b009b36",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Total number of elements in flat_list_of_tokens: {len(flat_list_of_tokens)}\")\n",
    "\n",
    "# Check for any sublists\n",
    "for i, item in enumerate(flat_list_of_tokens):\n",
    "    if isinstance(item, list):\n",
    "        print(f\"Found a sublist at index {i}: {item}\")\n",
    "        break\n",
    "else:\n",
    "    print(\"The list is flat.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54040684",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "flat_list = [item for sublist in flat_list_of_tokens for item in (sublist if isinstance(sublist, list) else [sublist])]\n",
    "\n",
    "# Now you can use np.unique on the flattened list\n",
    "unique_tokens, counts = np.unique(flat_list, return_counts=True)\n",
    "\n",
    "#sum(counts)\n",
    "unique_tokens[:9560]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "20e660fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import random\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class SiameseLSTM(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_layers):\n",
    "        super(SiameseLSTM, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers, batch_first=True, bidirectional=True)\n",
    "        self.cosine_similarity = nn.CosineSimilarity(dim=1, eps=1e-6)\n",
    "\n",
    "    def forward(self, query, doc):\n",
    "        query_embedding = self.embedding(query)\n",
    "        doc_embedding = self.embedding(doc)\n",
    "\n",
    "        # Get the outputs and the last hidden state from the LSTM\n",
    "        query_output, (query_hidden, _) = self.lstm(query_embedding)\n",
    "        doc_output, (doc_hidden, _) = self.lstm(doc_embedding)\n",
    "\n",
    "        # Since the LSTM is bidirectional, we need to concatenate the final forward\n",
    "        # and backward hidden states\n",
    "        query_hidden = torch.cat((query_hidden[-2,:,:], query_hidden[-1,:,:]), dim = 1)\n",
    "        doc_hidden = torch.cat((doc_hidden[-2,:,:], doc_hidden[-1,:,:]), dim = 1)\n",
    "\n",
    "        # Calculate cosine similarity on the final hidden state vectors\n",
    "        similarity = self.cosine_similarity(query_hidden, doc_hidden)\n",
    "        return similarity\n",
    "\n",
    "    def contrastive_loss(self, positive_similarity, negative_similarities, margin=0.5):\n",
    "        losses = [F.relu(margin - positive_similarity + neg_sim) for neg_sim in negative_similarities]\n",
    "        loss = sum(losses) / len(losses)\n",
    "        return loss.mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "aa1a3ab9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mpelumia23\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.12"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/pelumi/Project/language_detector/learn_to_search/learn_to_search/wandb/run-20231105_161934-j258r5q6</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/pelumia23/my_siamese_project/runs/j258r5q6' target=\"_blank\">unique-mountain-14</a></strong> to <a href='https://wandb.ai/pelumia23/my_siamese_project' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/pelumia23/my_siamese_project' target=\"_blank\">https://wandb.ai/pelumia23/my_siamese_project</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/pelumia23/my_siamese_project/runs/j258r5q6' target=\"_blank\">https://wandb.ai/pelumia23/my_siamese_project/runs/j258r5q6</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/pelumia23/my_siamese_project/runs/j258r5q6?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7f1424769750>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "wandb.init(project='my_siamese_project', entity='pelumia23')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bc54b17b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "what is a apothem\n",
      "Apothem of a Regular Polygon. Definition: A line segment from the center of a regular polygon to the midpoint of a side. Try this Adjust the polygon below by dragging any orange dot, or alter the number of sides. Note the behavior of the apothem line shown in blue. The word apothem can refer to the line itself, or the length of that line. So you can correctly say 'draw the apothem' and 'the apothem is 4cm'. Each formula below shows how to find the length of the apothem of a regular polygon. Use the formula that uses the facts you are given to start.\n"
     ]
    }
   ],
   "source": [
    "filtered_queries_n = []\n",
    "filtered_passages_n = []\n",
    "filtered_labels = []\n",
    "filtered_cleaned_paragraphs = []\n",
    "filtered_query = []\n",
    "for i, encoded_query in enumerate(padded_queries):\n",
    "    relevant_docs_indices = df['cleaned_passages_with_labels'][i]\n",
    "    if relevant_docs_indices:  # Only keep queries with relevant documents\n",
    "        filtered_queries_n.append(encoded_query)\n",
    "        filtered_passages_n.append(padded_passages[i])\n",
    "        filtered_query.append(df['query'][i])\n",
    "        filtered_cleaned_paragraphs.append(df['cleaned_paragraphs'][i])\n",
    "        filtered_labels.append(relevant_docs_indices)\n",
    "print(filtered_query[2])\n",
    "print(filtered_cleaned_paragraphs[2][2])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4fce2273",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: Mean Loss = 0.42928335806261747\n"
     ]
    }
   ],
   "source": [
    "vocab_size = 3000\n",
    "embedding_dim = 50\n",
    "hidden_dim = 64\n",
    "num_layers = 2\n",
    "siamese_model = SiameseLSTM(vocab_size, embedding_dim, hidden_dim, num_layers)\n",
    "optimizer = optim.Adam(siamese_model.parameters(), lr=0.01)\n",
    "num_epochs = 3\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    siamese_model.train()  # Set the model to training mode\n",
    "    total_loss = 0.0\n",
    "    total_iterations = 0\n",
    "\n",
    "    for i, encoded_query in enumerate(filtered_queries_n[:500]):\n",
    "        query_tensor = torch.LongTensor(encoded_query)\n",
    "        relevant_doc_index = filtered_labels[i][0]  # Get the index of the relevant document\n",
    "        positive_doc_tensor = torch.LongTensor(filtered_passages_n[i][relevant_doc_index])  # Index directly with the relevant doc index\n",
    "\n",
    "        negative_indices = random.sample([x for x in range(len(filtered_queries_n)) if x != i], 4)\n",
    "        negative_indices.append(i)  # Ensure one negative sample comes from the same query index\n",
    "\n",
    "        n_similarities = []\n",
    "        for neg_idx in negative_indices:\n",
    "                if neg_idx == i:\n",
    "                    # Avoid choosing the relevant document as a negative sample\n",
    "                    non_relevant_docs = [idx for idx in range(len(filtered_passages_n[neg_idx])) if idx != relevant_doc_index]\n",
    "                    if not non_relevant_docs:\n",
    "                        continue  # Skip if no non-relevant docs are available\n",
    "                    doc_idx = random.choice(non_relevant_docs)\n",
    "                else:\n",
    "                    doc_idx = random.choice(range(len(filtered_passages_n[neg_idx])))\n",
    "\n",
    "                negative_doc_tensor = torch.LongTensor(filtered_passages_n[neg_idx][doc_idx])\n",
    "\n",
    "                # Calculate similarity\n",
    "                n_similarity = siamese_model(query_tensor, negative_doc_tensor)\n",
    "                n_similarities.append(n_similarity)\n",
    "\n",
    "        # Calculate the positive similarity\n",
    "        positive_similarity = siamese_model(query_tensor, positive_doc_tensor)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss = siamese_model.contrastive_loss(positive_similarity, n_similarities)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        wandb.log({'batch_loss': loss.item(), 'epoch': epoch})\n",
    "        total_iterations += 1\n",
    "        \n",
    "mean_loss = total_loss / total_iterations if total_iterations > 0 else 0\n",
    "print(f\"Epoch {epoch + 1}: Mean Loss = {mean_loss}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e6e1b9ee",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def rank_documents_for_query(model, query_index, queries, docs):\n",
    "    \"\"\"\n",
    "    Rank documents based on their similarity to a query using a Siamese LSTM model.\n",
    "\n",
    "    Parameters:\n",
    "    - model: A trained SiameseLSTM model.\n",
    "    - query_index: The index of the query in the queries list.\n",
    "    - queries: A list of queries, each represented as a tensor.\n",
    "    - docs: A list of documents, each represented as a tensor.\n",
    "\n",
    "    Returns:\n",
    "    - A list of tuples (doc_index, similarity_score) sorted by similarity score in descending order.\n",
    "    \"\"\"\n",
    "    model.eval()  # Put the model in evaluation mode\n",
    "    query_tensor = torch.LongTensor(queries[query_index])  # Convert to tensor and add batch dimension\n",
    "\n",
    "    similarities = []\n",
    "    with torch.no_grad():  # No need to track gradients for evaluation\n",
    "        for i, doc in enumerate(docs):\n",
    "            doc_tensor = torch.LongTensor(doc)  # Convert to tensor and add batch dimension\n",
    "            similarity_tensor = model(query_tensor, doc_tensor)\n",
    "            # Ensure the tensor is a single value tensor\n",
    "            if similarity_tensor.numel() == 1:\n",
    "                similarity = similarity_tensor.item()\n",
    "            else:\n",
    "                # If the similarity tensor has more than one value, take the mean\n",
    "                similarity = similarity_tensor.mean().item()\n",
    "            similarities.append((i, similarity))\n",
    "\n",
    "    # Sort the documents by similarity score in descending order\n",
    "    ranked_docs_with_scores = sorted(similarities, key=lambda x: x[1], reverse=True)\n",
    "    return ranked_docs_with_scores\n",
    "\n",
    "# ... rest of your code ...\n",
    "\n",
    "# When printing the results:\n",
    "\n",
    "\n",
    "# Assuming `siamese_model` is your trained SiameseLSTM model instance,\n",
    "# `query_tensor` is the tensor for your single query,\n",
    "# and `doc_tensors` is a list of tensors for the documents to rank\n",
    "query_index = 1200\n",
    "ranked_docs_with_scores = rank_documents_for_query(siamese_model, query_index, filtered_queries_n, filtered_passages_n[query_index])\n",
    "\n",
    "\n",
    "def truncate_text_by_words(text, max_words=20):\n",
    "    words = text.split()\n",
    "    return ' '.join(words[:max_words]) + ('...' if len(words) > max_words else '')\n",
    "\n",
    "# Example usage with word truncation:\n",
    "#print(f\"Query: {truncate_text_by_words(filtered_query[query_index])}\")\n",
    "#for doc_index, score in ranked_docs_with_scores:\n",
    "    #doc_text = filtered_cleaned_paragraphs[query_index][doc_index]\n",
    "    #print(f\"Document {doc_index}: Score {score:.4f} | Text: {truncate_text_by_words(doc_text)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7c5cf2da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: what is a cheetah's natural habitat\n",
      "Document 4: Score 0.9345 | Text: The cheetah (Acinonyx jubatus) is a big cat in the subfamily Felinae that inhabits most of Africa and parts of...\n",
      "Document 8: Score 0.9211 | Text: The cheetah runs down its prey and therefore needs open spaces across which to do this. they enjoy eating frogs...\n",
      "Document 1: Score 0.9030 | Text: Here are some of the most interesting and amazing cheetah facts for kids. The cheetah (Acinonyx jubatus) is a feline...\n",
      "Document 7: Score 0.8808 | Text: Cheetahs are well studied in their natural habitat; however, studying cheetah biology in the wild remains difficult. Nuances of cheetah...\n",
      "Document 5: Score 0.8798 | Text: Cheetahs are carnivores, meaning their primary food is meat. The predators stick to smaller prey, such as gazelles, hares, young...\n",
      "Document 3: Score 0.8600 | Text: Cheetahs may be fast but the question that wildlife biologists ask is, are they fast enough to outrun extinction? Today...\n",
      "Document 0: Score 0.4243 | Text: Threats to Cheetahs. The cheetah’s future is uncertain due to a variety of threats. The biggest is habitat loss due...\n",
      "Document 2: Score 0.3692 | Text: In addition, cheetahs are believed to be in direct competition with the local ranchers, which makes them constant targets for...\n",
      "Document 6: Score 0.2656 | Text: 1 With a narrow waist and slim body, cheetah’s body is covered by several black-circled spots that measure 2 –...\n"
     ]
    }
   ],
   "source": [
    "print(f\"Query: {truncate_text_by_words(filtered_query[query_index])}\")\n",
    "for doc_index, score in ranked_docs_with_scores:\n",
    "    doc_text = filtered_cleaned_paragraphs[query_index][doc_index]\n",
    "    # Format the score as float\n",
    "    print(f\"Document {doc_index}: Score {float(score):.4f} | Text: {truncate_text_by_words(doc_text)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "156086a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, doc in enumerate(filtered_cleaned_paragraphs[query_index]):\n",
    "    print(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a16df174",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
